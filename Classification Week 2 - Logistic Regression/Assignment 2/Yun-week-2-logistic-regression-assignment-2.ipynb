{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with L2 regularization\n",
    "\n",
    "The goal of this second notebook is to implement your own logistic regression classifier with L2 regularization. You will do the following:\n",
    "\n",
    " * Extract features from Amazon product reviews.\n",
    " * Convert an SFrame into a NumPy array.\n",
    " * Write a function to compute the derivative of log likelihood function with an L2 penalty with respect to a single coefficient.\n",
    " * Implement gradient ascent with an L2 penalty.\n",
    " * Empirically explore how the L2 penalty can ameliorate overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fire up Scikit-learn, Pandas and Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import datasets, linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load review dataset\n",
    "\n",
    "For this assignment, we will use the same subset of the Amazon product review dataset that we used in Module 3 assignment. The subset was chosen to contain similar numbers of positive and negative reviews, as the original dataset consisted of mostly positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products = pd.read_csv('amazon_baby_subset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.. Just like we did previously, we will work with a hand-curated list of important words extracted from the review data. We will also perform 2 simple data transformations:\n",
    "\n",
    "~Remove punctuation\n",
    "\n",
    "~Compute word counts (only for the important_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the first item as follows:\n",
    "\n",
    "If your tool supports it, fill n/a values in the review column with empty strings. The n/a values indicate empty reviews. For instance, Pandas's the fillna() method lets you replace all N/A's in the review columns as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products = products.fillna({'review':''})  # fill in N/A's in the review column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function remove_punctuation that takes a line of text and removes all punctuation from that text. The function should be analogous to the following Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    import string\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the remove_punctuation function on every element of the review column and assign the result to the new column review_clean. Note. Many data frame packages support apply operation for this type of task. Consult appropriate manuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products['review_clean'] = products['review'].astype(str).apply(remove_punctuation) #astype(str) makes sure all reviews are strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.. Now we proceed with the second item. For each word in important_words, we compute a count for the number of times the word occurs in the review. We will store this count in a separate column (one for each word). The result of this feature processing is a single column for each word in important_words which keeps a count of the number of times the respective word occurs in the review text.\n",
    "\n",
    "Note: There are several ways of doing this. One way is to create an anonymous function that counts the occurrence of a particular word and apply it to every element in the review_clean column. Repeat this step for every word in important_words. Your code should be analogous to the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('important_words.json', 'r') as f: # Reads the list of most frequent words\n",
    "    important_words = json.load(f)\n",
    "important_words = [str(s) for s in important_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in important_words:\n",
    "    products[word] = products['review_clean'].apply(lambda s : s.split().count(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.. After #2 and #3, the data frame products should contain one column for each of the 193 important_words. As an example, the column perfect contains a count of the number of times the word prefect occurs in each of the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    1\n",
       "4    0\n",
       "Name: perfect, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['perfect'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Validation split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read the index documents\n",
    "traindata = pd.read_json('module-4-assignment-train-idx.json')\n",
    "validdata = pd.read_json('module-4-assignment-validation-idx.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pull train and test data from products data using the index docs\n",
    "train_data = products.loc[traindata.values.reshape((-1,))]\n",
    "validation_data = products.loc[validdata.values.reshape((-1,))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert DataFrame to NumPy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.. Convert train_data and validation_data into multi-dimensional arrays.\n",
    "\n",
    "Using the function given in #8 of Module 3 assignment, extract two arrays feature_matrix_train and sentiment_train from train_data. The 2D array feature_matrix_train would contain the content of the columns given by the list important_words. The 1D array sentiment_train would contain the content of the column sentiment. Do the same for validation_data, producing the arrays feature_matrix_valid and sentiment_valid. The code should be analogous to this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_numpy_data(dataframe, features, label):\n",
    "    dataframe['constant'] = 1\n",
    "    features = ['constant'] + features\n",
    "    #print(features)\n",
    "    features_frame = dataframe[features]\n",
    "    #print(dataframe[features])\n",
    "    #print(features_frame)\n",
    "    feature_matrix = features_frame.values\n",
    "    label_sarray = dataframe[label]\n",
    "    label_array = label_sarray.values\n",
    "    return(feature_matrix, label_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_matrix_train, sentiment_train = get_numpy_data(train_data, important_words, 'sentiment')\n",
    "feature_matrix_valid, sentiment_valid = get_numpy_data(validation_data, important_words, 'sentiment') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building on logistic regression with no L2 penalty assignment\n",
    "\n",
    "Let us now build on Module 3 assignment. Recall from lecture that the link function for logistic regression can be defined as:\n",
    "\n",
    "$$\n",
    "P(y_i = +1 | \\mathbf{x}_i,\\mathbf{w}) = \\frac{1}{1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))},\n",
    "$$\n",
    "\n",
    "where the feature vector $h(\\mathbf{x}_i)$ is given by the word counts of **important_words** in the review $\\mathbf{x}_i$. \n",
    "\n",
    "We will use the **same code** as in this past assignment to make probability predictions since this part is not affected by the L2 penalty.  (Only the way in which the coefficients are learned is affected by the addition of a regularization term.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_probability(feature_matrix, coefficients):\n",
    "    # Take dot product of feature_matrix and coefficients  \n",
    "    # YOUR CODE HERE\n",
    "    score = np.dot(feature_matrix, coefficients)\n",
    "    \n",
    "    # Compute P(y_i = +1 | x_i, w) using the link function\n",
    "    # YOUR CODE HERE\n",
    "    predictions = 1/(1+(np.exp(-score)))\n",
    "    \n",
    "    # return predictions\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding  L2 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that computes the derivative of log likelihood with respect to a single coefficient $w_j$. Unlike its counterpart in the last assignment, the function accepts five arguments:\n",
    " * `errors` vector containing $(\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w}))$ for all $i$\n",
    " * `feature` vector containing $h_j(\\mathbf{x}_i)$  for all $i$\n",
    " * `coefficient` containing the current value of coefficient $w_j$.\n",
    " * `l2_penalty` representing the L2 penalty constant $\\lambda$\n",
    " * `feature_is_constant` telling whether the $j$-th feature is constant or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_derivative_with_L2(errors, feature, coefficient, l2_penalty, feature_is_constant): \n",
    "    \n",
    "    # Compute the dot product of errors and feature\n",
    "    ## YOUR CODE HERE\n",
    "    derivative = np.dot(errors, feature)\n",
    "\n",
    "    # add L2 penalty term for any feature that isn't the intercept.\n",
    "    if not feature_is_constant: \n",
    "        ## YOUR CODE HERE\n",
    "        derivative = derivative-2*l2_penalty*coefficient\n",
    "        \n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiz question: In the code above, was the intercept term regularized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is: no\n"
     ]
    }
   ],
   "source": [
    "print('The answer is: no')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.. To verify the correctness of the gradient descent algorithm, we write a function for computing log likelihood (which we recall from the last assignment was a topic detailed in an advanced optional video, and used here for its numerical stability), which is given by the formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_log_likelihood_with_L2(feature_matrix, sentiment, coefficients, l2_penalty):\n",
    "    indicator = (sentiment==+1)\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    \n",
    "    lp = np.sum((indicator-1)*scores - np.log(1. + np.exp(-scores))) - l2_penalty*np.sum(coefficients[1:]**2)\n",
    "    \n",
    "    return lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quiz question: Does the term with L2 regularization increase or decrease ℓℓ(w)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is: decrease\n"
     ]
    }
   ],
   "source": [
    "print('The answer is: decrease')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11.. The logistic regression function looks almost like the one in the last assignment, with a minor modification to account for the L2 penalty.\n",
    "\n",
    "Write a function logistic_regression_with_L2 to fit a logistic regression model under L2 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_with_L2(feature_matrix, sentiment, initial_coefficients, step_size, l2_penalty, max_iter):\n",
    "    coefficients = np.array(initial_coefficients) # make sure it's a numpy array\n",
    "    for itr in range(max_iter):\n",
    "        # Predict P(y_i = +1|x_i,w) using your predict_probability() function\n",
    "        ## YOUR CODE HERE\n",
    "        predictions = predict_probability(feature_matrix, coefficients)\n",
    "        \n",
    "        # Compute indicator value for (y_i = +1)\n",
    "        indicator = (sentiment==+1)\n",
    "        \n",
    "        # Compute the errors as indicator - predictions\n",
    "        errors = indicator - predictions\n",
    "        for j in range(len(coefficients)): # loop over each coefficient\n",
    "            is_intercept = (j == 0)\n",
    "            # Recall that feature_matrix[:,j] is the feature column associated with coefficients[j].\n",
    "            # Compute the derivative for coefficients[j]. Save it in a variable called derivative\n",
    "            ## YOUR CODE HERE\n",
    "            derivative = feature_derivative_with_L2(errors, feature_matrix[:,j], coefficients[j], l2_penalty, is_intercept)\n",
    "            #print('derivative', derivative)\n",
    "            # add the step size times the derivative to the current coefficient\n",
    "            ## YOUR CODE HERE\n",
    "            coefficients[j] = coefficients[j] + step_size*derivative\n",
    "        \n",
    "        # Checking whether log likelihood is increasing\n",
    "        if itr <= 15 or (itr <= 100 and itr % 10 == 0) or (itr <= 1000 and itr % 100 == 0) \\\n",
    "        or (itr <= 10000 and itr % 1000 == 0) or itr % 10000 == 0:\n",
    "            lp = compute_log_likelihood_with_L2(feature_matrix, sentiment, coefficients, l2_penalty)\n",
    "            print ('iteration %*d: log likelihood of observed labels = %.8f' % \\\n",
    "                (int(np.ceil(np.log10(max_iter))), itr, lp))\n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore effects of L2 regularization\n",
    "\n",
    "Now that we have written up all the pieces needed for regularized logistic regression, let's explore the benefits of using **L2 regularization** in analyzing sentiment for product reviews. **As iterations pass, the log likelihood should increase**.\n",
    "\n",
    "Below, we train models with increasing amounts of regularization, starting with no L2 penalty, which is equivalent to our previous logistic regression implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.. Now that we have written up all the pieces needed for an L2 solver with logistic regression, let's explore the benefits of using L2 regularization while analyzing sentiment for product reviews. As iterations pass, the log likelihood should increase.\n",
    "\n",
    "Let us train models with increasing amounts of regularization, starting with no L2 penalty, which is equivalent to our previous logistic regression implementation. Train 6 models with L2 penalty values 0, 4, 10, 1e2, 1e3, and 1e5. Use the following values for the other parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature_matrix = feature_matrix_train extracted in #7\n",
    "\n",
    "sentiment = sentiment_train extracted in #7\n",
    "\n",
    "initial_coefficients = a 194-dimensional vector filled with zeros\n",
    "\n",
    "step_size = 5e-6\n",
    "\n",
    "max_iter = 501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29179.39138303\n",
      "iteration   1: log likelihood of observed labels = -29003.71259047\n",
      "iteration   2: log likelihood of observed labels = -28834.66187288\n",
      "iteration   3: log likelihood of observed labels = -28671.70781507\n",
      "iteration   4: log likelihood of observed labels = -28514.43078198\n",
      "iteration   5: log likelihood of observed labels = -28362.48344665\n",
      "iteration   6: log likelihood of observed labels = -28215.56713122\n",
      "iteration   7: log likelihood of observed labels = -28073.41743783\n",
      "iteration   8: log likelihood of observed labels = -27935.79536396\n",
      "iteration   9: log likelihood of observed labels = -27802.48168669\n",
      "iteration  10: log likelihood of observed labels = -27673.27331484\n",
      "iteration  11: log likelihood of observed labels = -27547.98083656\n",
      "iteration  12: log likelihood of observed labels = -27426.42679977\n",
      "iteration  13: log likelihood of observed labels = -27308.44444728\n",
      "iteration  14: log likelihood of observed labels = -27193.87673876\n",
      "iteration  15: log likelihood of observed labels = -27082.57555831\n",
      "iteration  20: log likelihood of observed labels = -26570.43059938\n",
      "iteration  30: log likelihood of observed labels = -25725.48742389\n",
      "iteration  40: log likelihood of observed labels = -25055.53326910\n",
      "iteration  50: log likelihood of observed labels = -24509.63590026\n",
      "iteration  60: log likelihood of observed labels = -24054.97906083\n",
      "iteration  70: log likelihood of observed labels = -23669.51640848\n",
      "iteration  80: log likelihood of observed labels = -23337.89167628\n",
      "iteration  90: log likelihood of observed labels = -23049.07066021\n",
      "iteration 100: log likelihood of observed labels = -22794.90974921\n",
      "iteration 200: log likelihood of observed labels = -21283.29527353\n",
      "iteration 300: log likelihood of observed labels = -20570.97485473\n",
      "iteration 400: log likelihood of observed labels = -20152.21466944\n",
      "iteration 500: log likelihood of observed labels = -19876.62333410\n"
     ]
    }
   ],
   "source": [
    "# run with L2 = 0\n",
    "coefficients_0_penalty = logistic_regression_with_L2(feature_matrix_train, sentiment_train,\n",
    "                                                     initial_coefficients=np.zeros(194),\n",
    "                                                     step_size=5e-6, l2_penalty=0, max_iter=501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29179.39508175\n",
      "iteration   1: log likelihood of observed labels = -29003.73417180\n",
      "iteration   2: log likelihood of observed labels = -28834.71441858\n",
      "iteration   3: log likelihood of observed labels = -28671.80345068\n",
      "iteration   4: log likelihood of observed labels = -28514.58077957\n",
      "iteration   5: log likelihood of observed labels = -28362.69830317\n",
      "iteration   6: log likelihood of observed labels = -28215.85663259\n",
      "iteration   7: log likelihood of observed labels = -28073.79071393\n",
      "iteration   8: log likelihood of observed labels = -27936.26093762\n",
      "iteration   9: log likelihood of observed labels = -27803.04751805\n",
      "iteration  10: log likelihood of observed labels = -27673.94684207\n",
      "iteration  11: log likelihood of observed labels = -27548.76901327\n",
      "iteration  12: log likelihood of observed labels = -27427.33612958\n",
      "iteration  13: log likelihood of observed labels = -27309.48101569\n",
      "iteration  14: log likelihood of observed labels = -27195.04624253\n",
      "iteration  15: log likelihood of observed labels = -27083.88333261\n",
      "iteration  20: log likelihood of observed labels = -26572.49874392\n",
      "iteration  30: log likelihood of observed labels = -25729.32604153\n",
      "iteration  40: log likelihood of observed labels = -25061.34245801\n",
      "iteration  50: log likelihood of observed labels = -24517.52091982\n",
      "iteration  60: log likelihood of observed labels = -24064.99093939\n",
      "iteration  70: log likelihood of observed labels = -23681.67373669\n",
      "iteration  80: log likelihood of observed labels = -23352.19298741\n",
      "iteration  90: log likelihood of observed labels = -23065.50180166\n",
      "iteration 100: log likelihood of observed labels = -22813.44844580\n",
      "iteration 200: log likelihood of observed labels = -21321.14164794\n",
      "iteration 300: log likelihood of observed labels = -20624.98634439\n",
      "iteration 400: log likelihood of observed labels = -20219.92048845\n",
      "iteration 500: log likelihood of observed labels = -19956.11341777\n"
     ]
    }
   ],
   "source": [
    "# run with L2 = 4\n",
    "coefficients_4_penalty = logistic_regression_with_L2(feature_matrix_train, sentiment_train,\n",
    "                                                      initial_coefficients=np.zeros(194),\n",
    "                                                      step_size=5e-6, l2_penalty=4, max_iter=501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29179.40062984\n",
      "iteration   1: log likelihood of observed labels = -29003.76654163\n",
      "iteration   2: log likelihood of observed labels = -28834.79322654\n",
      "iteration   3: log likelihood of observed labels = -28671.94687528\n",
      "iteration   4: log likelihood of observed labels = -28514.80571589\n",
      "iteration   5: log likelihood of observed labels = -28363.02048079\n",
      "iteration   6: log likelihood of observed labels = -28216.29071186\n",
      "iteration   7: log likelihood of observed labels = -28074.35036891\n",
      "iteration   8: log likelihood of observed labels = -27936.95892966\n",
      "iteration   9: log likelihood of observed labels = -27803.89576265\n",
      "iteration  10: log likelihood of observed labels = -27674.95647005\n",
      "iteration  11: log likelihood of observed labels = -27549.95042714\n",
      "iteration  12: log likelihood of observed labels = -27428.69905549\n",
      "iteration  13: log likelihood of observed labels = -27311.03455140\n",
      "iteration  14: log likelihood of observed labels = -27196.79890162\n",
      "iteration  15: log likelihood of observed labels = -27085.84308528\n",
      "iteration  20: log likelihood of observed labels = -26575.59697506\n",
      "iteration  30: log likelihood of observed labels = -25735.07304608\n",
      "iteration  40: log likelihood of observed labels = -25070.03447306\n",
      "iteration  50: log likelihood of observed labels = -24529.31188025\n",
      "iteration  60: log likelihood of observed labels = -24079.95349572\n",
      "iteration  70: log likelihood of observed labels = -23699.83199186\n",
      "iteration  80: log likelihood of observed labels = -23373.54108747\n",
      "iteration  90: log likelihood of observed labels = -23090.01500055\n",
      "iteration 100: log likelihood of observed labels = -22841.08995135\n",
      "iteration 200: log likelihood of observed labels = -21377.25595328\n",
      "iteration 300: log likelihood of observed labels = -20704.63995428\n",
      "iteration 400: log likelihood of observed labels = -20319.25685307\n",
      "iteration 500: log likelihood of observed labels = -20072.16321721\n"
     ]
    }
   ],
   "source": [
    "# run with L2 = 10\n",
    "coefficients_10_penalty = logistic_regression_with_L2(feature_matrix_train, sentiment_train,\n",
    "                                                      initial_coefficients=np.zeros(194),\n",
    "                                                      step_size=5e-6, l2_penalty=10, max_iter=501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29179.48385120\n",
      "iteration   1: log likelihood of observed labels = -29004.25177457\n",
      "iteration   2: log likelihood of observed labels = -28835.97382190\n",
      "iteration   3: log likelihood of observed labels = -28674.09410083\n",
      "iteration   4: log likelihood of observed labels = -28518.17112932\n",
      "iteration   5: log likelihood of observed labels = -28367.83774654\n",
      "iteration   6: log likelihood of observed labels = -28222.77708939\n",
      "iteration   7: log likelihood of observed labels = -28082.70799392\n",
      "iteration   8: log likelihood of observed labels = -27947.37595368\n",
      "iteration   9: log likelihood of observed labels = -27816.54738615\n",
      "iteration  10: log likelihood of observed labels = -27690.00588850\n",
      "iteration  11: log likelihood of observed labels = -27567.54970126\n",
      "iteration  12: log likelihood of observed labels = -27448.98991327\n",
      "iteration  13: log likelihood of observed labels = -27334.14912742\n",
      "iteration  14: log likelihood of observed labels = -27222.86041863\n",
      "iteration  15: log likelihood of observed labels = -27114.96648229\n",
      "iteration  20: log likelihood of observed labels = -26621.50201299\n",
      "iteration  30: log likelihood of observed labels = -25819.72803950\n",
      "iteration  40: log likelihood of observed labels = -25197.34035501\n",
      "iteration  50: log likelihood of observed labels = -24701.03698195\n",
      "iteration  60: log likelihood of observed labels = -24296.66378580\n",
      "iteration  70: log likelihood of observed labels = -23961.38842316\n",
      "iteration  80: log likelihood of observed labels = -23679.38088853\n",
      "iteration  90: log likelihood of observed labels = -23439.31824267\n",
      "iteration 100: log likelihood of observed labels = -23232.88192018\n",
      "iteration 200: log likelihood of observed labels = -22133.50726528\n",
      "iteration 300: log likelihood of observed labels = -21730.03957488\n",
      "iteration 400: log likelihood of observed labels = -21545.87572145\n",
      "iteration 500: log likelihood of observed labels = -21451.95551390\n"
     ]
    }
   ],
   "source": [
    "# run with L2 = 1e2\n",
    "coefficients_1e2_penalty = logistic_regression_with_L2(feature_matrix_train, sentiment_train,\n",
    "                                                       initial_coefficients=np.zeros(194),\n",
    "                                                       step_size=5e-6, l2_penalty=1e2, max_iter=501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29180.31606471\n",
      "iteration   1: log likelihood of observed labels = -29009.07176112\n",
      "iteration   2: log likelihood of observed labels = -28847.62378912\n",
      "iteration   3: log likelihood of observed labels = -28695.14439397\n",
      "iteration   4: log likelihood of observed labels = -28550.95060743\n",
      "iteration   5: log likelihood of observed labels = -28414.45771129\n",
      "iteration   6: log likelihood of observed labels = -28285.15124375\n",
      "iteration   7: log likelihood of observed labels = -28162.56976044\n",
      "iteration   8: log likelihood of observed labels = -28046.29387744\n",
      "iteration   9: log likelihood of observed labels = -27935.93902900\n",
      "iteration  10: log likelihood of observed labels = -27831.15045502\n",
      "iteration  11: log likelihood of observed labels = -27731.59955260\n",
      "iteration  12: log likelihood of observed labels = -27636.98108219\n",
      "iteration  13: log likelihood of observed labels = -27547.01092670\n",
      "iteration  14: log likelihood of observed labels = -27461.42422295\n",
      "iteration  15: log likelihood of observed labels = -27379.97375625\n",
      "iteration  20: log likelihood of observed labels = -27027.18208317\n",
      "iteration  30: log likelihood of observed labels = -26527.22737267\n",
      "iteration  40: log likelihood of observed labels = -26206.59048765\n",
      "iteration  50: log likelihood of observed labels = -25995.96903148\n",
      "iteration  60: log likelihood of observed labels = -25854.95710284\n",
      "iteration  70: log likelihood of observed labels = -25759.08109950\n",
      "iteration  80: log likelihood of observed labels = -25693.05688014\n",
      "iteration  90: log likelihood of observed labels = -25647.09929349\n",
      "iteration 100: log likelihood of observed labels = -25614.81468705\n",
      "iteration 200: log likelihood of observed labels = -25536.20998919\n",
      "iteration 300: log likelihood of observed labels = -25532.57691220\n",
      "iteration 400: log likelihood of observed labels = -25532.35543765\n",
      "iteration 500: log likelihood of observed labels = -25532.33970049\n"
     ]
    }
   ],
   "source": [
    "# run with L2 = 1e3\n",
    "coefficients_1e3_penalty = logistic_regression_with_L2(feature_matrix_train, sentiment_train,\n",
    "                                                       initial_coefficients=np.zeros(194),\n",
    "                                                       step_size=5e-6, l2_penalty=1e3, max_iter=501)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -29271.85955115\n",
      "iteration   1: log likelihood of observed labels = -29271.71006589\n",
      "iteration   2: log likelihood of observed labels = -29271.65738833\n",
      "iteration   3: log likelihood of observed labels = -29271.61189923\n",
      "iteration   4: log likelihood of observed labels = -29271.57079975\n",
      "iteration   5: log likelihood of observed labels = -29271.53358505\n",
      "iteration   6: log likelihood of observed labels = -29271.49988440\n",
      "iteration   7: log likelihood of observed labels = -29271.46936584\n",
      "iteration   8: log likelihood of observed labels = -29271.44172890\n",
      "iteration   9: log likelihood of observed labels = -29271.41670149\n",
      "iteration  10: log likelihood of observed labels = -29271.39403722\n",
      "iteration  11: log likelihood of observed labels = -29271.37351294\n",
      "iteration  12: log likelihood of observed labels = -29271.35492661\n",
      "iteration  13: log likelihood of observed labels = -29271.33809523\n",
      "iteration  14: log likelihood of observed labels = -29271.32285309\n",
      "iteration  15: log likelihood of observed labels = -29271.30905015\n",
      "iteration  20: log likelihood of observed labels = -29271.25729150\n",
      "iteration  30: log likelihood of observed labels = -29271.20657205\n",
      "iteration  40: log likelihood of observed labels = -29271.18775997\n",
      "iteration  50: log likelihood of observed labels = -29271.18078247\n",
      "iteration  60: log likelihood of observed labels = -29271.17819447\n",
      "iteration  70: log likelihood of observed labels = -29271.17723457\n",
      "iteration  80: log likelihood of observed labels = -29271.17687853\n",
      "iteration  90: log likelihood of observed labels = -29271.17674648\n",
      "iteration 100: log likelihood of observed labels = -29271.17669750\n",
      "iteration 200: log likelihood of observed labels = -29271.17666862\n",
      "iteration 300: log likelihood of observed labels = -29271.17666862\n",
      "iteration 400: log likelihood of observed labels = -29271.17666862\n",
      "iteration 500: log likelihood of observed labels = -29271.17666862\n"
     ]
    }
   ],
   "source": [
    "# run with L2 = 1e5\n",
    "coefficients_1e5_penalty = logistic_regression_with_L2(feature_matrix_train, sentiment_train,\n",
    "                                                       initial_coefficients=np.zeros(194),\n",
    "                                                       step_size=5e-6, l2_penalty=1e5, max_iter=501)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare coefficients\n",
    "\n",
    "We now compare the **coefficients** for each of the models that were trained above. We will create a table of features and learned coefficients associated with each of the different L2 penalty values.\n",
    "\n",
    "Below is a simple helper function that will help us create this table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table = pd.DataFrame({'word': ['(intercept)'] + important_words})\n",
    "def add_coefficients_to_table(coefficients, column_name):\n",
    "    table[column_name] = coefficients\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coefficients [L2=0]</th>\n",
       "      <th>coefficients [L2=4]</th>\n",
       "      <th>coefficients [L2=10]</th>\n",
       "      <th>coefficients [L2=1e2]</th>\n",
       "      <th>coefficients [L2=1e3]</th>\n",
       "      <th>coefficients [L2=1e5]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(intercept)</td>\n",
       "      <td>-0.063742</td>\n",
       "      <td>-0.063143</td>\n",
       "      <td>-0.062256</td>\n",
       "      <td>-0.050438</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.011362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baby</td>\n",
       "      <td>0.074073</td>\n",
       "      <td>0.073994</td>\n",
       "      <td>0.073877</td>\n",
       "      <td>0.072360</td>\n",
       "      <td>0.059752</td>\n",
       "      <td>0.001784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>one</td>\n",
       "      <td>0.012753</td>\n",
       "      <td>0.012495</td>\n",
       "      <td>0.012115</td>\n",
       "      <td>0.007247</td>\n",
       "      <td>-0.008761</td>\n",
       "      <td>-0.001827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>great</td>\n",
       "      <td>0.801625</td>\n",
       "      <td>0.796897</td>\n",
       "      <td>0.789935</td>\n",
       "      <td>0.701425</td>\n",
       "      <td>0.376012</td>\n",
       "      <td>0.008950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>love</td>\n",
       "      <td>1.058554</td>\n",
       "      <td>1.050856</td>\n",
       "      <td>1.039529</td>\n",
       "      <td>0.896644</td>\n",
       "      <td>0.418354</td>\n",
       "      <td>0.009042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>use</td>\n",
       "      <td>-0.000104</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.005481</td>\n",
       "      <td>0.017326</td>\n",
       "      <td>0.000418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>would</td>\n",
       "      <td>-0.287021</td>\n",
       "      <td>-0.286027</td>\n",
       "      <td>-0.284564</td>\n",
       "      <td>-0.265993</td>\n",
       "      <td>-0.188662</td>\n",
       "      <td>-0.008127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>like</td>\n",
       "      <td>-0.003384</td>\n",
       "      <td>-0.003442</td>\n",
       "      <td>-0.003527</td>\n",
       "      <td>-0.004635</td>\n",
       "      <td>-0.007043</td>\n",
       "      <td>-0.000827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>easy</td>\n",
       "      <td>0.984559</td>\n",
       "      <td>0.977600</td>\n",
       "      <td>0.967362</td>\n",
       "      <td>0.838245</td>\n",
       "      <td>0.401904</td>\n",
       "      <td>0.008808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>little</td>\n",
       "      <td>0.524419</td>\n",
       "      <td>0.521385</td>\n",
       "      <td>0.516917</td>\n",
       "      <td>0.460235</td>\n",
       "      <td>0.251221</td>\n",
       "      <td>0.005941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>seat</td>\n",
       "      <td>-0.086968</td>\n",
       "      <td>-0.086125</td>\n",
       "      <td>-0.084883</td>\n",
       "      <td>-0.069109</td>\n",
       "      <td>-0.017718</td>\n",
       "      <td>0.000611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>old</td>\n",
       "      <td>0.208912</td>\n",
       "      <td>0.207749</td>\n",
       "      <td>0.206037</td>\n",
       "      <td>0.184332</td>\n",
       "      <td>0.105074</td>\n",
       "      <td>0.002741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>well</td>\n",
       "      <td>0.453866</td>\n",
       "      <td>0.450969</td>\n",
       "      <td>0.446700</td>\n",
       "      <td>0.392304</td>\n",
       "      <td>0.194926</td>\n",
       "      <td>0.003945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>get</td>\n",
       "      <td>-0.196835</td>\n",
       "      <td>-0.196100</td>\n",
       "      <td>-0.195017</td>\n",
       "      <td>-0.181251</td>\n",
       "      <td>-0.122728</td>\n",
       "      <td>-0.004578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>also</td>\n",
       "      <td>0.158163</td>\n",
       "      <td>0.157246</td>\n",
       "      <td>0.155899</td>\n",
       "      <td>0.139153</td>\n",
       "      <td>0.080918</td>\n",
       "      <td>0.001929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>really</td>\n",
       "      <td>-0.017906</td>\n",
       "      <td>-0.017745</td>\n",
       "      <td>-0.017508</td>\n",
       "      <td>-0.014481</td>\n",
       "      <td>-0.004448</td>\n",
       "      <td>-0.000340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>son</td>\n",
       "      <td>0.128396</td>\n",
       "      <td>0.127761</td>\n",
       "      <td>0.126828</td>\n",
       "      <td>0.115192</td>\n",
       "      <td>0.070411</td>\n",
       "      <td>0.001552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>time</td>\n",
       "      <td>-0.072429</td>\n",
       "      <td>-0.072281</td>\n",
       "      <td>-0.072065</td>\n",
       "      <td>-0.069480</td>\n",
       "      <td>-0.057581</td>\n",
       "      <td>-0.002805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bought</td>\n",
       "      <td>-0.151817</td>\n",
       "      <td>-0.150917</td>\n",
       "      <td>-0.149594</td>\n",
       "      <td>-0.132884</td>\n",
       "      <td>-0.072431</td>\n",
       "      <td>-0.001985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>product</td>\n",
       "      <td>-0.263330</td>\n",
       "      <td>-0.262328</td>\n",
       "      <td>-0.260854</td>\n",
       "      <td>-0.242391</td>\n",
       "      <td>-0.167962</td>\n",
       "      <td>-0.006211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>good</td>\n",
       "      <td>0.156507</td>\n",
       "      <td>0.155270</td>\n",
       "      <td>0.153445</td>\n",
       "      <td>0.129972</td>\n",
       "      <td>0.047879</td>\n",
       "      <td>0.000266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>daughter</td>\n",
       "      <td>0.263418</td>\n",
       "      <td>0.261775</td>\n",
       "      <td>0.259357</td>\n",
       "      <td>0.228685</td>\n",
       "      <td>0.117158</td>\n",
       "      <td>0.002401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>much</td>\n",
       "      <td>-0.013247</td>\n",
       "      <td>-0.013295</td>\n",
       "      <td>-0.013366</td>\n",
       "      <td>-0.014326</td>\n",
       "      <td>-0.015219</td>\n",
       "      <td>-0.000839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>loves</td>\n",
       "      <td>1.052484</td>\n",
       "      <td>1.043903</td>\n",
       "      <td>1.031265</td>\n",
       "      <td>0.870794</td>\n",
       "      <td>0.345870</td>\n",
       "      <td>0.006150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>stroller</td>\n",
       "      <td>-0.037533</td>\n",
       "      <td>-0.036988</td>\n",
       "      <td>-0.036186</td>\n",
       "      <td>-0.025990</td>\n",
       "      <td>0.005912</td>\n",
       "      <td>0.001326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>put</td>\n",
       "      <td>-0.000330</td>\n",
       "      <td>-0.000323</td>\n",
       "      <td>-0.000312</td>\n",
       "      <td>-0.000127</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>-0.000097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>months</td>\n",
       "      <td>-0.067995</td>\n",
       "      <td>-0.067315</td>\n",
       "      <td>-0.066314</td>\n",
       "      <td>-0.053594</td>\n",
       "      <td>-0.013083</td>\n",
       "      <td>-0.000157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>car</td>\n",
       "      <td>0.193364</td>\n",
       "      <td>0.191904</td>\n",
       "      <td>0.189754</td>\n",
       "      <td>0.162531</td>\n",
       "      <td>0.072719</td>\n",
       "      <td>0.001765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>still</td>\n",
       "      <td>0.188508</td>\n",
       "      <td>0.187071</td>\n",
       "      <td>0.184955</td>\n",
       "      <td>0.158163</td>\n",
       "      <td>0.068491</td>\n",
       "      <td>0.000976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>back</td>\n",
       "      <td>-0.268954</td>\n",
       "      <td>-0.267419</td>\n",
       "      <td>-0.265161</td>\n",
       "      <td>-0.236730</td>\n",
       "      <td>-0.134671</td>\n",
       "      <td>-0.003988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>started</td>\n",
       "      <td>-0.153174</td>\n",
       "      <td>-0.151852</td>\n",
       "      <td>-0.149905</td>\n",
       "      <td>-0.125084</td>\n",
       "      <td>-0.045084</td>\n",
       "      <td>-0.000877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>anything</td>\n",
       "      <td>-0.186801</td>\n",
       "      <td>-0.185242</td>\n",
       "      <td>-0.182943</td>\n",
       "      <td>-0.153602</td>\n",
       "      <td>-0.057284</td>\n",
       "      <td>-0.001053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>last</td>\n",
       "      <td>-0.099469</td>\n",
       "      <td>-0.098692</td>\n",
       "      <td>-0.097547</td>\n",
       "      <td>-0.083001</td>\n",
       "      <td>-0.034797</td>\n",
       "      <td>-0.000775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>company</td>\n",
       "      <td>-0.276548</td>\n",
       "      <td>-0.274151</td>\n",
       "      <td>-0.270621</td>\n",
       "      <td>-0.225839</td>\n",
       "      <td>-0.084898</td>\n",
       "      <td>-0.001719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>come</td>\n",
       "      <td>-0.032009</td>\n",
       "      <td>-0.031804</td>\n",
       "      <td>-0.031502</td>\n",
       "      <td>-0.027685</td>\n",
       "      <td>-0.014185</td>\n",
       "      <td>-0.000426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>returned</td>\n",
       "      <td>-0.572707</td>\n",
       "      <td>-0.567518</td>\n",
       "      <td>-0.559870</td>\n",
       "      <td>-0.462056</td>\n",
       "      <td>-0.150021</td>\n",
       "      <td>-0.002225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>maybe</td>\n",
       "      <td>-0.224076</td>\n",
       "      <td>-0.222015</td>\n",
       "      <td>-0.218976</td>\n",
       "      <td>-0.180192</td>\n",
       "      <td>-0.058149</td>\n",
       "      <td>-0.000945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>took</td>\n",
       "      <td>-0.046445</td>\n",
       "      <td>-0.046199</td>\n",
       "      <td>-0.045838</td>\n",
       "      <td>-0.041422</td>\n",
       "      <td>-0.025566</td>\n",
       "      <td>-0.000772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>broke</td>\n",
       "      <td>-0.555195</td>\n",
       "      <td>-0.550209</td>\n",
       "      <td>-0.542861</td>\n",
       "      <td>-0.448989</td>\n",
       "      <td>-0.148726</td>\n",
       "      <td>-0.002182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>makes</td>\n",
       "      <td>-0.009023</td>\n",
       "      <td>-0.008764</td>\n",
       "      <td>-0.008382</td>\n",
       "      <td>-0.003467</td>\n",
       "      <td>0.008757</td>\n",
       "      <td>0.000255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>stay</td>\n",
       "      <td>-0.300563</td>\n",
       "      <td>-0.297920</td>\n",
       "      <td>-0.294024</td>\n",
       "      <td>-0.244247</td>\n",
       "      <td>-0.083709</td>\n",
       "      <td>-0.001310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>instead</td>\n",
       "      <td>-0.193123</td>\n",
       "      <td>-0.191418</td>\n",
       "      <td>-0.188907</td>\n",
       "      <td>-0.156863</td>\n",
       "      <td>-0.054125</td>\n",
       "      <td>-0.000925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>idea</td>\n",
       "      <td>-0.465370</td>\n",
       "      <td>-0.461130</td>\n",
       "      <td>-0.454879</td>\n",
       "      <td>-0.374890</td>\n",
       "      <td>-0.118469</td>\n",
       "      <td>-0.001627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>head</td>\n",
       "      <td>-0.110472</td>\n",
       "      <td>-0.109559</td>\n",
       "      <td>-0.108215</td>\n",
       "      <td>-0.090992</td>\n",
       "      <td>-0.032986</td>\n",
       "      <td>-0.000502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>said</td>\n",
       "      <td>-0.098049</td>\n",
       "      <td>-0.097331</td>\n",
       "      <td>-0.096274</td>\n",
       "      <td>-0.082875</td>\n",
       "      <td>-0.037594</td>\n",
       "      <td>-0.000947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>less</td>\n",
       "      <td>-0.136801</td>\n",
       "      <td>-0.135652</td>\n",
       "      <td>-0.133958</td>\n",
       "      <td>-0.112360</td>\n",
       "      <td>-0.042260</td>\n",
       "      <td>-0.000873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>went</td>\n",
       "      <td>-0.106836</td>\n",
       "      <td>-0.106003</td>\n",
       "      <td>-0.104776</td>\n",
       "      <td>-0.089294</td>\n",
       "      <td>-0.039417</td>\n",
       "      <td>-0.001006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>working</td>\n",
       "      <td>-0.320363</td>\n",
       "      <td>-0.317559</td>\n",
       "      <td>-0.313427</td>\n",
       "      <td>-0.260764</td>\n",
       "      <td>-0.092334</td>\n",
       "      <td>-0.001674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>high</td>\n",
       "      <td>0.003326</td>\n",
       "      <td>0.003282</td>\n",
       "      <td>0.003217</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>-0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>unit</td>\n",
       "      <td>-0.196121</td>\n",
       "      <td>-0.194516</td>\n",
       "      <td>-0.192153</td>\n",
       "      <td>-0.162210</td>\n",
       "      <td>-0.066568</td>\n",
       "      <td>-0.001567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>seems</td>\n",
       "      <td>0.058308</td>\n",
       "      <td>0.057905</td>\n",
       "      <td>0.057312</td>\n",
       "      <td>0.049753</td>\n",
       "      <td>0.022875</td>\n",
       "      <td>0.000329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>picture</td>\n",
       "      <td>-0.196906</td>\n",
       "      <td>-0.195273</td>\n",
       "      <td>-0.192866</td>\n",
       "      <td>-0.162143</td>\n",
       "      <td>-0.061171</td>\n",
       "      <td>-0.001151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>completely</td>\n",
       "      <td>-0.277845</td>\n",
       "      <td>-0.275461</td>\n",
       "      <td>-0.271947</td>\n",
       "      <td>-0.227098</td>\n",
       "      <td>-0.081775</td>\n",
       "      <td>-0.001421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>wish</td>\n",
       "      <td>0.173191</td>\n",
       "      <td>0.171640</td>\n",
       "      <td>0.169352</td>\n",
       "      <td>0.140022</td>\n",
       "      <td>0.044374</td>\n",
       "      <td>0.000468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>buying</td>\n",
       "      <td>-0.132197</td>\n",
       "      <td>-0.131083</td>\n",
       "      <td>-0.129441</td>\n",
       "      <td>-0.108471</td>\n",
       "      <td>-0.040331</td>\n",
       "      <td>-0.000792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>babies</td>\n",
       "      <td>0.052494</td>\n",
       "      <td>0.052130</td>\n",
       "      <td>0.051594</td>\n",
       "      <td>0.044805</td>\n",
       "      <td>0.021026</td>\n",
       "      <td>0.000365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>won</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>0.004907</td>\n",
       "      <td>0.004830</td>\n",
       "      <td>0.003848</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>tub</td>\n",
       "      <td>-0.166745</td>\n",
       "      <td>-0.165367</td>\n",
       "      <td>-0.163338</td>\n",
       "      <td>-0.137693</td>\n",
       "      <td>-0.054778</td>\n",
       "      <td>-0.000936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>almost</td>\n",
       "      <td>-0.031916</td>\n",
       "      <td>-0.031621</td>\n",
       "      <td>-0.031186</td>\n",
       "      <td>-0.025604</td>\n",
       "      <td>-0.007361</td>\n",
       "      <td>-0.000125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>either</td>\n",
       "      <td>-0.228852</td>\n",
       "      <td>-0.226793</td>\n",
       "      <td>-0.223758</td>\n",
       "      <td>-0.184986</td>\n",
       "      <td>-0.061138</td>\n",
       "      <td>-0.000980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  coefficients [L2=0]  coefficients [L2=4]  \\\n",
       "0    (intercept)            -0.063742            -0.063143   \n",
       "1           baby             0.074073             0.073994   \n",
       "2            one             0.012753             0.012495   \n",
       "3          great             0.801625             0.796897   \n",
       "4           love             1.058554             1.050856   \n",
       "5            use            -0.000104             0.000163   \n",
       "6          would            -0.287021            -0.286027   \n",
       "7           like            -0.003384            -0.003442   \n",
       "8           easy             0.984559             0.977600   \n",
       "9         little             0.524419             0.521385   \n",
       "10          seat            -0.086968            -0.086125   \n",
       "11           old             0.208912             0.207749   \n",
       "12          well             0.453866             0.450969   \n",
       "13           get            -0.196835            -0.196100   \n",
       "14          also             0.158163             0.157246   \n",
       "15        really            -0.017906            -0.017745   \n",
       "16           son             0.128396             0.127761   \n",
       "17          time            -0.072429            -0.072281   \n",
       "18        bought            -0.151817            -0.150917   \n",
       "19       product            -0.263330            -0.262328   \n",
       "20          good             0.156507             0.155270   \n",
       "21      daughter             0.263418             0.261775   \n",
       "22          much            -0.013247            -0.013295   \n",
       "23         loves             1.052484             1.043903   \n",
       "24      stroller            -0.037533            -0.036988   \n",
       "25           put            -0.000330            -0.000323   \n",
       "26        months            -0.067995            -0.067315   \n",
       "27           car             0.193364             0.191904   \n",
       "28         still             0.188508             0.187071   \n",
       "29          back            -0.268954            -0.267419   \n",
       "..           ...                  ...                  ...   \n",
       "164      started            -0.153174            -0.151852   \n",
       "165     anything            -0.186801            -0.185242   \n",
       "166         last            -0.099469            -0.098692   \n",
       "167      company            -0.276548            -0.274151   \n",
       "168         come            -0.032009            -0.031804   \n",
       "169     returned            -0.572707            -0.567518   \n",
       "170        maybe            -0.224076            -0.222015   \n",
       "171         took            -0.046445            -0.046199   \n",
       "172        broke            -0.555195            -0.550209   \n",
       "173        makes            -0.009023            -0.008764   \n",
       "174         stay            -0.300563            -0.297920   \n",
       "175      instead            -0.193123            -0.191418   \n",
       "176         idea            -0.465370            -0.461130   \n",
       "177         head            -0.110472            -0.109559   \n",
       "178         said            -0.098049            -0.097331   \n",
       "179         less            -0.136801            -0.135652   \n",
       "180         went            -0.106836            -0.106003   \n",
       "181      working            -0.320363            -0.317559   \n",
       "182         high             0.003326             0.003282   \n",
       "183         unit            -0.196121            -0.194516   \n",
       "184        seems             0.058308             0.057905   \n",
       "185      picture            -0.196906            -0.195273   \n",
       "186   completely            -0.277845            -0.275461   \n",
       "187         wish             0.173191             0.171640   \n",
       "188       buying            -0.132197            -0.131083   \n",
       "189       babies             0.052494             0.052130   \n",
       "190          won             0.004960             0.004907   \n",
       "191          tub            -0.166745            -0.165367   \n",
       "192       almost            -0.031916            -0.031621   \n",
       "193       either            -0.228852            -0.226793   \n",
       "\n",
       "     coefficients [L2=10]  coefficients [L2=1e2]  coefficients [L2=1e3]  \\\n",
       "0               -0.062256              -0.050438               0.000054   \n",
       "1                0.073877               0.072360               0.059752   \n",
       "2                0.012115               0.007247              -0.008761   \n",
       "3                0.789935               0.701425               0.376012   \n",
       "4                1.039529               0.896644               0.418354   \n",
       "5                0.000556               0.005481               0.017326   \n",
       "6               -0.284564              -0.265993              -0.188662   \n",
       "7               -0.003527              -0.004635              -0.007043   \n",
       "8                0.967362               0.838245               0.401904   \n",
       "9                0.516917               0.460235               0.251221   \n",
       "10              -0.084883              -0.069109              -0.017718   \n",
       "11               0.206037               0.184332               0.105074   \n",
       "12               0.446700               0.392304               0.194926   \n",
       "13              -0.195017              -0.181251              -0.122728   \n",
       "14               0.155899               0.139153               0.080918   \n",
       "15              -0.017508              -0.014481              -0.004448   \n",
       "16               0.126828               0.115192               0.070411   \n",
       "17              -0.072065              -0.069480              -0.057581   \n",
       "18              -0.149594              -0.132884              -0.072431   \n",
       "19              -0.260854              -0.242391              -0.167962   \n",
       "20               0.153445               0.129972               0.047879   \n",
       "21               0.259357               0.228685               0.117158   \n",
       "22              -0.013366              -0.014326              -0.015219   \n",
       "23               1.031265               0.870794               0.345870   \n",
       "24              -0.036186              -0.025990               0.005912   \n",
       "25              -0.000312              -0.000127               0.001529   \n",
       "26              -0.066314              -0.053594              -0.013083   \n",
       "27               0.189754               0.162531               0.072719   \n",
       "28               0.184955               0.158163               0.068491   \n",
       "29              -0.265161              -0.236730              -0.134671   \n",
       "..                    ...                    ...                    ...   \n",
       "164             -0.149905              -0.125084              -0.045084   \n",
       "165             -0.182943              -0.153602              -0.057284   \n",
       "166             -0.097547              -0.083001              -0.034797   \n",
       "167             -0.270621              -0.225839              -0.084898   \n",
       "168             -0.031502              -0.027685              -0.014185   \n",
       "169             -0.559870              -0.462056              -0.150021   \n",
       "170             -0.218976              -0.180192              -0.058149   \n",
       "171             -0.045838              -0.041422              -0.025566   \n",
       "172             -0.542861              -0.448989              -0.148726   \n",
       "173             -0.008382              -0.003467               0.008757   \n",
       "174             -0.294024              -0.244247              -0.083709   \n",
       "175             -0.188907              -0.156863              -0.054125   \n",
       "176             -0.454879              -0.374890              -0.118469   \n",
       "177             -0.108215              -0.090992              -0.032986   \n",
       "178             -0.096274              -0.082875              -0.037594   \n",
       "179             -0.133958              -0.112360              -0.042260   \n",
       "180             -0.104776              -0.089294              -0.039417   \n",
       "181             -0.313427              -0.260764              -0.092334   \n",
       "182              0.003217               0.002404               0.000236   \n",
       "183             -0.192153              -0.162210              -0.066568   \n",
       "184              0.057312               0.049753               0.022875   \n",
       "185             -0.192866              -0.162143              -0.061171   \n",
       "186             -0.271947              -0.227098              -0.081775   \n",
       "187              0.169352               0.140022               0.044374   \n",
       "188             -0.129441              -0.108471              -0.040331   \n",
       "189              0.051594               0.044805               0.021026   \n",
       "190              0.004830               0.003848               0.001084   \n",
       "191             -0.163338              -0.137693              -0.054778   \n",
       "192             -0.031186              -0.025604              -0.007361   \n",
       "193             -0.223758              -0.184986              -0.061138   \n",
       "\n",
       "     coefficients [L2=1e5]  \n",
       "0                 0.011362  \n",
       "1                 0.001784  \n",
       "2                -0.001827  \n",
       "3                 0.008950  \n",
       "4                 0.009042  \n",
       "5                 0.000418  \n",
       "6                -0.008127  \n",
       "7                -0.000827  \n",
       "8                 0.008808  \n",
       "9                 0.005941  \n",
       "10                0.000611  \n",
       "11                0.002741  \n",
       "12                0.003945  \n",
       "13               -0.004578  \n",
       "14                0.001929  \n",
       "15               -0.000340  \n",
       "16                0.001552  \n",
       "17               -0.002805  \n",
       "18               -0.001985  \n",
       "19               -0.006211  \n",
       "20                0.000266  \n",
       "21                0.002401  \n",
       "22               -0.000839  \n",
       "23                0.006150  \n",
       "24                0.001326  \n",
       "25               -0.000097  \n",
       "26               -0.000157  \n",
       "27                0.001765  \n",
       "28                0.000976  \n",
       "29               -0.003988  \n",
       "..                     ...  \n",
       "164              -0.000877  \n",
       "165              -0.001053  \n",
       "166              -0.000775  \n",
       "167              -0.001719  \n",
       "168              -0.000426  \n",
       "169              -0.002225  \n",
       "170              -0.000945  \n",
       "171              -0.000772  \n",
       "172              -0.002182  \n",
       "173               0.000255  \n",
       "174              -0.001310  \n",
       "175              -0.000925  \n",
       "176              -0.001627  \n",
       "177              -0.000502  \n",
       "178              -0.000947  \n",
       "179              -0.000873  \n",
       "180              -0.001006  \n",
       "181              -0.001674  \n",
       "182              -0.000062  \n",
       "183              -0.001567  \n",
       "184               0.000329  \n",
       "185              -0.001151  \n",
       "186              -0.001421  \n",
       "187               0.000468  \n",
       "188              -0.000792  \n",
       "189               0.000365  \n",
       "190               0.000017  \n",
       "191              -0.000936  \n",
       "192              -0.000125  \n",
       "193              -0.000980  \n",
       "\n",
       "[194 rows x 7 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_coefficients_to_table(coefficients_0_penalty, 'coefficients [L2=0]')\n",
    "add_coefficients_to_table(coefficients_4_penalty, 'coefficients [L2=4]')\n",
    "add_coefficients_to_table(coefficients_10_penalty, 'coefficients [L2=10]')\n",
    "add_coefficients_to_table(coefficients_1e2_penalty, 'coefficients [L2=1e2]')\n",
    "add_coefficients_to_table(coefficients_1e3_penalty, 'coefficients [L2=1e3]')\n",
    "add_coefficients_to_table(coefficients_1e5_penalty, 'coefficients [L2=1e5]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coefficients [L2=0]</th>\n",
       "      <th>coefficients [L2=4]</th>\n",
       "      <th>coefficients [L2=10]</th>\n",
       "      <th>coefficients [L2=1e2]</th>\n",
       "      <th>coefficients [L2=1e3]</th>\n",
       "      <th>coefficients [L2=1e5]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(intercept)</td>\n",
       "      <td>-0.063742</td>\n",
       "      <td>-0.063143</td>\n",
       "      <td>-0.062256</td>\n",
       "      <td>-0.050438</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.011362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>baby</td>\n",
       "      <td>0.074073</td>\n",
       "      <td>0.073994</td>\n",
       "      <td>0.073877</td>\n",
       "      <td>0.072360</td>\n",
       "      <td>0.059752</td>\n",
       "      <td>0.001784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>one</td>\n",
       "      <td>0.012753</td>\n",
       "      <td>0.012495</td>\n",
       "      <td>0.012115</td>\n",
       "      <td>0.007247</td>\n",
       "      <td>-0.008761</td>\n",
       "      <td>-0.001827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>great</td>\n",
       "      <td>0.801625</td>\n",
       "      <td>0.796897</td>\n",
       "      <td>0.789935</td>\n",
       "      <td>0.701425</td>\n",
       "      <td>0.376012</td>\n",
       "      <td>0.008950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>love</td>\n",
       "      <td>1.058554</td>\n",
       "      <td>1.050856</td>\n",
       "      <td>1.039529</td>\n",
       "      <td>0.896644</td>\n",
       "      <td>0.418354</td>\n",
       "      <td>0.009042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>use</td>\n",
       "      <td>-0.000104</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.005481</td>\n",
       "      <td>0.017326</td>\n",
       "      <td>0.000418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>would</td>\n",
       "      <td>-0.287021</td>\n",
       "      <td>-0.286027</td>\n",
       "      <td>-0.284564</td>\n",
       "      <td>-0.265993</td>\n",
       "      <td>-0.188662</td>\n",
       "      <td>-0.008127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>like</td>\n",
       "      <td>-0.003384</td>\n",
       "      <td>-0.003442</td>\n",
       "      <td>-0.003527</td>\n",
       "      <td>-0.004635</td>\n",
       "      <td>-0.007043</td>\n",
       "      <td>-0.000827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>easy</td>\n",
       "      <td>0.984559</td>\n",
       "      <td>0.977600</td>\n",
       "      <td>0.967362</td>\n",
       "      <td>0.838245</td>\n",
       "      <td>0.401904</td>\n",
       "      <td>0.008808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>little</td>\n",
       "      <td>0.524419</td>\n",
       "      <td>0.521385</td>\n",
       "      <td>0.516917</td>\n",
       "      <td>0.460235</td>\n",
       "      <td>0.251221</td>\n",
       "      <td>0.005941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>seat</td>\n",
       "      <td>-0.086968</td>\n",
       "      <td>-0.086125</td>\n",
       "      <td>-0.084883</td>\n",
       "      <td>-0.069109</td>\n",
       "      <td>-0.017718</td>\n",
       "      <td>0.000611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>old</td>\n",
       "      <td>0.208912</td>\n",
       "      <td>0.207749</td>\n",
       "      <td>0.206037</td>\n",
       "      <td>0.184332</td>\n",
       "      <td>0.105074</td>\n",
       "      <td>0.002741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>well</td>\n",
       "      <td>0.453866</td>\n",
       "      <td>0.450969</td>\n",
       "      <td>0.446700</td>\n",
       "      <td>0.392304</td>\n",
       "      <td>0.194926</td>\n",
       "      <td>0.003945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>get</td>\n",
       "      <td>-0.196835</td>\n",
       "      <td>-0.196100</td>\n",
       "      <td>-0.195017</td>\n",
       "      <td>-0.181251</td>\n",
       "      <td>-0.122728</td>\n",
       "      <td>-0.004578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>also</td>\n",
       "      <td>0.158163</td>\n",
       "      <td>0.157246</td>\n",
       "      <td>0.155899</td>\n",
       "      <td>0.139153</td>\n",
       "      <td>0.080918</td>\n",
       "      <td>0.001929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>really</td>\n",
       "      <td>-0.017906</td>\n",
       "      <td>-0.017745</td>\n",
       "      <td>-0.017508</td>\n",
       "      <td>-0.014481</td>\n",
       "      <td>-0.004448</td>\n",
       "      <td>-0.000340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>son</td>\n",
       "      <td>0.128396</td>\n",
       "      <td>0.127761</td>\n",
       "      <td>0.126828</td>\n",
       "      <td>0.115192</td>\n",
       "      <td>0.070411</td>\n",
       "      <td>0.001552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>time</td>\n",
       "      <td>-0.072429</td>\n",
       "      <td>-0.072281</td>\n",
       "      <td>-0.072065</td>\n",
       "      <td>-0.069480</td>\n",
       "      <td>-0.057581</td>\n",
       "      <td>-0.002805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>bought</td>\n",
       "      <td>-0.151817</td>\n",
       "      <td>-0.150917</td>\n",
       "      <td>-0.149594</td>\n",
       "      <td>-0.132884</td>\n",
       "      <td>-0.072431</td>\n",
       "      <td>-0.001985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>product</td>\n",
       "      <td>-0.263330</td>\n",
       "      <td>-0.262328</td>\n",
       "      <td>-0.260854</td>\n",
       "      <td>-0.242391</td>\n",
       "      <td>-0.167962</td>\n",
       "      <td>-0.006211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>good</td>\n",
       "      <td>0.156507</td>\n",
       "      <td>0.155270</td>\n",
       "      <td>0.153445</td>\n",
       "      <td>0.129972</td>\n",
       "      <td>0.047879</td>\n",
       "      <td>0.000266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>daughter</td>\n",
       "      <td>0.263418</td>\n",
       "      <td>0.261775</td>\n",
       "      <td>0.259357</td>\n",
       "      <td>0.228685</td>\n",
       "      <td>0.117158</td>\n",
       "      <td>0.002401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>much</td>\n",
       "      <td>-0.013247</td>\n",
       "      <td>-0.013295</td>\n",
       "      <td>-0.013366</td>\n",
       "      <td>-0.014326</td>\n",
       "      <td>-0.015219</td>\n",
       "      <td>-0.000839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>loves</td>\n",
       "      <td>1.052484</td>\n",
       "      <td>1.043903</td>\n",
       "      <td>1.031265</td>\n",
       "      <td>0.870794</td>\n",
       "      <td>0.345870</td>\n",
       "      <td>0.006150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>stroller</td>\n",
       "      <td>-0.037533</td>\n",
       "      <td>-0.036988</td>\n",
       "      <td>-0.036186</td>\n",
       "      <td>-0.025990</td>\n",
       "      <td>0.005912</td>\n",
       "      <td>0.001326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>put</td>\n",
       "      <td>-0.000330</td>\n",
       "      <td>-0.000323</td>\n",
       "      <td>-0.000312</td>\n",
       "      <td>-0.000127</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>-0.000097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>months</td>\n",
       "      <td>-0.067995</td>\n",
       "      <td>-0.067315</td>\n",
       "      <td>-0.066314</td>\n",
       "      <td>-0.053594</td>\n",
       "      <td>-0.013083</td>\n",
       "      <td>-0.000157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>car</td>\n",
       "      <td>0.193364</td>\n",
       "      <td>0.191904</td>\n",
       "      <td>0.189754</td>\n",
       "      <td>0.162531</td>\n",
       "      <td>0.072719</td>\n",
       "      <td>0.001765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>still</td>\n",
       "      <td>0.188508</td>\n",
       "      <td>0.187071</td>\n",
       "      <td>0.184955</td>\n",
       "      <td>0.158163</td>\n",
       "      <td>0.068491</td>\n",
       "      <td>0.000976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>back</td>\n",
       "      <td>-0.268954</td>\n",
       "      <td>-0.267419</td>\n",
       "      <td>-0.265161</td>\n",
       "      <td>-0.236730</td>\n",
       "      <td>-0.134671</td>\n",
       "      <td>-0.003988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>started</td>\n",
       "      <td>-0.153174</td>\n",
       "      <td>-0.151852</td>\n",
       "      <td>-0.149905</td>\n",
       "      <td>-0.125084</td>\n",
       "      <td>-0.045084</td>\n",
       "      <td>-0.000877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>anything</td>\n",
       "      <td>-0.186801</td>\n",
       "      <td>-0.185242</td>\n",
       "      <td>-0.182943</td>\n",
       "      <td>-0.153602</td>\n",
       "      <td>-0.057284</td>\n",
       "      <td>-0.001053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>last</td>\n",
       "      <td>-0.099469</td>\n",
       "      <td>-0.098692</td>\n",
       "      <td>-0.097547</td>\n",
       "      <td>-0.083001</td>\n",
       "      <td>-0.034797</td>\n",
       "      <td>-0.000775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>company</td>\n",
       "      <td>-0.276548</td>\n",
       "      <td>-0.274151</td>\n",
       "      <td>-0.270621</td>\n",
       "      <td>-0.225839</td>\n",
       "      <td>-0.084898</td>\n",
       "      <td>-0.001719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>come</td>\n",
       "      <td>-0.032009</td>\n",
       "      <td>-0.031804</td>\n",
       "      <td>-0.031502</td>\n",
       "      <td>-0.027685</td>\n",
       "      <td>-0.014185</td>\n",
       "      <td>-0.000426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>returned</td>\n",
       "      <td>-0.572707</td>\n",
       "      <td>-0.567518</td>\n",
       "      <td>-0.559870</td>\n",
       "      <td>-0.462056</td>\n",
       "      <td>-0.150021</td>\n",
       "      <td>-0.002225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>maybe</td>\n",
       "      <td>-0.224076</td>\n",
       "      <td>-0.222015</td>\n",
       "      <td>-0.218976</td>\n",
       "      <td>-0.180192</td>\n",
       "      <td>-0.058149</td>\n",
       "      <td>-0.000945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>took</td>\n",
       "      <td>-0.046445</td>\n",
       "      <td>-0.046199</td>\n",
       "      <td>-0.045838</td>\n",
       "      <td>-0.041422</td>\n",
       "      <td>-0.025566</td>\n",
       "      <td>-0.000772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>broke</td>\n",
       "      <td>-0.555195</td>\n",
       "      <td>-0.550209</td>\n",
       "      <td>-0.542861</td>\n",
       "      <td>-0.448989</td>\n",
       "      <td>-0.148726</td>\n",
       "      <td>-0.002182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>makes</td>\n",
       "      <td>-0.009023</td>\n",
       "      <td>-0.008764</td>\n",
       "      <td>-0.008382</td>\n",
       "      <td>-0.003467</td>\n",
       "      <td>0.008757</td>\n",
       "      <td>0.000255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>stay</td>\n",
       "      <td>-0.300563</td>\n",
       "      <td>-0.297920</td>\n",
       "      <td>-0.294024</td>\n",
       "      <td>-0.244247</td>\n",
       "      <td>-0.083709</td>\n",
       "      <td>-0.001310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>instead</td>\n",
       "      <td>-0.193123</td>\n",
       "      <td>-0.191418</td>\n",
       "      <td>-0.188907</td>\n",
       "      <td>-0.156863</td>\n",
       "      <td>-0.054125</td>\n",
       "      <td>-0.000925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>idea</td>\n",
       "      <td>-0.465370</td>\n",
       "      <td>-0.461130</td>\n",
       "      <td>-0.454879</td>\n",
       "      <td>-0.374890</td>\n",
       "      <td>-0.118469</td>\n",
       "      <td>-0.001627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>head</td>\n",
       "      <td>-0.110472</td>\n",
       "      <td>-0.109559</td>\n",
       "      <td>-0.108215</td>\n",
       "      <td>-0.090992</td>\n",
       "      <td>-0.032986</td>\n",
       "      <td>-0.000502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>said</td>\n",
       "      <td>-0.098049</td>\n",
       "      <td>-0.097331</td>\n",
       "      <td>-0.096274</td>\n",
       "      <td>-0.082875</td>\n",
       "      <td>-0.037594</td>\n",
       "      <td>-0.000947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>less</td>\n",
       "      <td>-0.136801</td>\n",
       "      <td>-0.135652</td>\n",
       "      <td>-0.133958</td>\n",
       "      <td>-0.112360</td>\n",
       "      <td>-0.042260</td>\n",
       "      <td>-0.000873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>went</td>\n",
       "      <td>-0.106836</td>\n",
       "      <td>-0.106003</td>\n",
       "      <td>-0.104776</td>\n",
       "      <td>-0.089294</td>\n",
       "      <td>-0.039417</td>\n",
       "      <td>-0.001006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>working</td>\n",
       "      <td>-0.320363</td>\n",
       "      <td>-0.317559</td>\n",
       "      <td>-0.313427</td>\n",
       "      <td>-0.260764</td>\n",
       "      <td>-0.092334</td>\n",
       "      <td>-0.001674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>high</td>\n",
       "      <td>0.003326</td>\n",
       "      <td>0.003282</td>\n",
       "      <td>0.003217</td>\n",
       "      <td>0.002404</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>-0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>unit</td>\n",
       "      <td>-0.196121</td>\n",
       "      <td>-0.194516</td>\n",
       "      <td>-0.192153</td>\n",
       "      <td>-0.162210</td>\n",
       "      <td>-0.066568</td>\n",
       "      <td>-0.001567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>seems</td>\n",
       "      <td>0.058308</td>\n",
       "      <td>0.057905</td>\n",
       "      <td>0.057312</td>\n",
       "      <td>0.049753</td>\n",
       "      <td>0.022875</td>\n",
       "      <td>0.000329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>picture</td>\n",
       "      <td>-0.196906</td>\n",
       "      <td>-0.195273</td>\n",
       "      <td>-0.192866</td>\n",
       "      <td>-0.162143</td>\n",
       "      <td>-0.061171</td>\n",
       "      <td>-0.001151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>completely</td>\n",
       "      <td>-0.277845</td>\n",
       "      <td>-0.275461</td>\n",
       "      <td>-0.271947</td>\n",
       "      <td>-0.227098</td>\n",
       "      <td>-0.081775</td>\n",
       "      <td>-0.001421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>wish</td>\n",
       "      <td>0.173191</td>\n",
       "      <td>0.171640</td>\n",
       "      <td>0.169352</td>\n",
       "      <td>0.140022</td>\n",
       "      <td>0.044374</td>\n",
       "      <td>0.000468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>buying</td>\n",
       "      <td>-0.132197</td>\n",
       "      <td>-0.131083</td>\n",
       "      <td>-0.129441</td>\n",
       "      <td>-0.108471</td>\n",
       "      <td>-0.040331</td>\n",
       "      <td>-0.000792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>babies</td>\n",
       "      <td>0.052494</td>\n",
       "      <td>0.052130</td>\n",
       "      <td>0.051594</td>\n",
       "      <td>0.044805</td>\n",
       "      <td>0.021026</td>\n",
       "      <td>0.000365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>won</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>0.004907</td>\n",
       "      <td>0.004830</td>\n",
       "      <td>0.003848</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>tub</td>\n",
       "      <td>-0.166745</td>\n",
       "      <td>-0.165367</td>\n",
       "      <td>-0.163338</td>\n",
       "      <td>-0.137693</td>\n",
       "      <td>-0.054778</td>\n",
       "      <td>-0.000936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>almost</td>\n",
       "      <td>-0.031916</td>\n",
       "      <td>-0.031621</td>\n",
       "      <td>-0.031186</td>\n",
       "      <td>-0.025604</td>\n",
       "      <td>-0.007361</td>\n",
       "      <td>-0.000125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>either</td>\n",
       "      <td>-0.228852</td>\n",
       "      <td>-0.226793</td>\n",
       "      <td>-0.223758</td>\n",
       "      <td>-0.184986</td>\n",
       "      <td>-0.061138</td>\n",
       "      <td>-0.000980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>194 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            word  coefficients [L2=0]  coefficients [L2=4]  \\\n",
       "0    (intercept)            -0.063742            -0.063143   \n",
       "1           baby             0.074073             0.073994   \n",
       "2            one             0.012753             0.012495   \n",
       "3          great             0.801625             0.796897   \n",
       "4           love             1.058554             1.050856   \n",
       "5            use            -0.000104             0.000163   \n",
       "6          would            -0.287021            -0.286027   \n",
       "7           like            -0.003384            -0.003442   \n",
       "8           easy             0.984559             0.977600   \n",
       "9         little             0.524419             0.521385   \n",
       "10          seat            -0.086968            -0.086125   \n",
       "11           old             0.208912             0.207749   \n",
       "12          well             0.453866             0.450969   \n",
       "13           get            -0.196835            -0.196100   \n",
       "14          also             0.158163             0.157246   \n",
       "15        really            -0.017906            -0.017745   \n",
       "16           son             0.128396             0.127761   \n",
       "17          time            -0.072429            -0.072281   \n",
       "18        bought            -0.151817            -0.150917   \n",
       "19       product            -0.263330            -0.262328   \n",
       "20          good             0.156507             0.155270   \n",
       "21      daughter             0.263418             0.261775   \n",
       "22          much            -0.013247            -0.013295   \n",
       "23         loves             1.052484             1.043903   \n",
       "24      stroller            -0.037533            -0.036988   \n",
       "25           put            -0.000330            -0.000323   \n",
       "26        months            -0.067995            -0.067315   \n",
       "27           car             0.193364             0.191904   \n",
       "28         still             0.188508             0.187071   \n",
       "29          back            -0.268954            -0.267419   \n",
       "..           ...                  ...                  ...   \n",
       "164      started            -0.153174            -0.151852   \n",
       "165     anything            -0.186801            -0.185242   \n",
       "166         last            -0.099469            -0.098692   \n",
       "167      company            -0.276548            -0.274151   \n",
       "168         come            -0.032009            -0.031804   \n",
       "169     returned            -0.572707            -0.567518   \n",
       "170        maybe            -0.224076            -0.222015   \n",
       "171         took            -0.046445            -0.046199   \n",
       "172        broke            -0.555195            -0.550209   \n",
       "173        makes            -0.009023            -0.008764   \n",
       "174         stay            -0.300563            -0.297920   \n",
       "175      instead            -0.193123            -0.191418   \n",
       "176         idea            -0.465370            -0.461130   \n",
       "177         head            -0.110472            -0.109559   \n",
       "178         said            -0.098049            -0.097331   \n",
       "179         less            -0.136801            -0.135652   \n",
       "180         went            -0.106836            -0.106003   \n",
       "181      working            -0.320363            -0.317559   \n",
       "182         high             0.003326             0.003282   \n",
       "183         unit            -0.196121            -0.194516   \n",
       "184        seems             0.058308             0.057905   \n",
       "185      picture            -0.196906            -0.195273   \n",
       "186   completely            -0.277845            -0.275461   \n",
       "187         wish             0.173191             0.171640   \n",
       "188       buying            -0.132197            -0.131083   \n",
       "189       babies             0.052494             0.052130   \n",
       "190          won             0.004960             0.004907   \n",
       "191          tub            -0.166745            -0.165367   \n",
       "192       almost            -0.031916            -0.031621   \n",
       "193       either            -0.228852            -0.226793   \n",
       "\n",
       "     coefficients [L2=10]  coefficients [L2=1e2]  coefficients [L2=1e3]  \\\n",
       "0               -0.062256              -0.050438               0.000054   \n",
       "1                0.073877               0.072360               0.059752   \n",
       "2                0.012115               0.007247              -0.008761   \n",
       "3                0.789935               0.701425               0.376012   \n",
       "4                1.039529               0.896644               0.418354   \n",
       "5                0.000556               0.005481               0.017326   \n",
       "6               -0.284564              -0.265993              -0.188662   \n",
       "7               -0.003527              -0.004635              -0.007043   \n",
       "8                0.967362               0.838245               0.401904   \n",
       "9                0.516917               0.460235               0.251221   \n",
       "10              -0.084883              -0.069109              -0.017718   \n",
       "11               0.206037               0.184332               0.105074   \n",
       "12               0.446700               0.392304               0.194926   \n",
       "13              -0.195017              -0.181251              -0.122728   \n",
       "14               0.155899               0.139153               0.080918   \n",
       "15              -0.017508              -0.014481              -0.004448   \n",
       "16               0.126828               0.115192               0.070411   \n",
       "17              -0.072065              -0.069480              -0.057581   \n",
       "18              -0.149594              -0.132884              -0.072431   \n",
       "19              -0.260854              -0.242391              -0.167962   \n",
       "20               0.153445               0.129972               0.047879   \n",
       "21               0.259357               0.228685               0.117158   \n",
       "22              -0.013366              -0.014326              -0.015219   \n",
       "23               1.031265               0.870794               0.345870   \n",
       "24              -0.036186              -0.025990               0.005912   \n",
       "25              -0.000312              -0.000127               0.001529   \n",
       "26              -0.066314              -0.053594              -0.013083   \n",
       "27               0.189754               0.162531               0.072719   \n",
       "28               0.184955               0.158163               0.068491   \n",
       "29              -0.265161              -0.236730              -0.134671   \n",
       "..                    ...                    ...                    ...   \n",
       "164             -0.149905              -0.125084              -0.045084   \n",
       "165             -0.182943              -0.153602              -0.057284   \n",
       "166             -0.097547              -0.083001              -0.034797   \n",
       "167             -0.270621              -0.225839              -0.084898   \n",
       "168             -0.031502              -0.027685              -0.014185   \n",
       "169             -0.559870              -0.462056              -0.150021   \n",
       "170             -0.218976              -0.180192              -0.058149   \n",
       "171             -0.045838              -0.041422              -0.025566   \n",
       "172             -0.542861              -0.448989              -0.148726   \n",
       "173             -0.008382              -0.003467               0.008757   \n",
       "174             -0.294024              -0.244247              -0.083709   \n",
       "175             -0.188907              -0.156863              -0.054125   \n",
       "176             -0.454879              -0.374890              -0.118469   \n",
       "177             -0.108215              -0.090992              -0.032986   \n",
       "178             -0.096274              -0.082875              -0.037594   \n",
       "179             -0.133958              -0.112360              -0.042260   \n",
       "180             -0.104776              -0.089294              -0.039417   \n",
       "181             -0.313427              -0.260764              -0.092334   \n",
       "182              0.003217               0.002404               0.000236   \n",
       "183             -0.192153              -0.162210              -0.066568   \n",
       "184              0.057312               0.049753               0.022875   \n",
       "185             -0.192866              -0.162143              -0.061171   \n",
       "186             -0.271947              -0.227098              -0.081775   \n",
       "187              0.169352               0.140022               0.044374   \n",
       "188             -0.129441              -0.108471              -0.040331   \n",
       "189              0.051594               0.044805               0.021026   \n",
       "190              0.004830               0.003848               0.001084   \n",
       "191             -0.163338              -0.137693              -0.054778   \n",
       "192             -0.031186              -0.025604              -0.007361   \n",
       "193             -0.223758              -0.184986              -0.061138   \n",
       "\n",
       "     coefficients [L2=1e5]  \n",
       "0                 0.011362  \n",
       "1                 0.001784  \n",
       "2                -0.001827  \n",
       "3                 0.008950  \n",
       "4                 0.009042  \n",
       "5                 0.000418  \n",
       "6                -0.008127  \n",
       "7                -0.000827  \n",
       "8                 0.008808  \n",
       "9                 0.005941  \n",
       "10                0.000611  \n",
       "11                0.002741  \n",
       "12                0.003945  \n",
       "13               -0.004578  \n",
       "14                0.001929  \n",
       "15               -0.000340  \n",
       "16                0.001552  \n",
       "17               -0.002805  \n",
       "18               -0.001985  \n",
       "19               -0.006211  \n",
       "20                0.000266  \n",
       "21                0.002401  \n",
       "22               -0.000839  \n",
       "23                0.006150  \n",
       "24                0.001326  \n",
       "25               -0.000097  \n",
       "26               -0.000157  \n",
       "27                0.001765  \n",
       "28                0.000976  \n",
       "29               -0.003988  \n",
       "..                     ...  \n",
       "164              -0.000877  \n",
       "165              -0.001053  \n",
       "166              -0.000775  \n",
       "167              -0.001719  \n",
       "168              -0.000426  \n",
       "169              -0.002225  \n",
       "170              -0.000945  \n",
       "171              -0.000772  \n",
       "172              -0.002182  \n",
       "173               0.000255  \n",
       "174              -0.001310  \n",
       "175              -0.000925  \n",
       "176              -0.001627  \n",
       "177              -0.000502  \n",
       "178              -0.000947  \n",
       "179              -0.000873  \n",
       "180              -0.001006  \n",
       "181              -0.001674  \n",
       "182              -0.000062  \n",
       "183              -0.001567  \n",
       "184               0.000329  \n",
       "185              -0.001151  \n",
       "186              -0.001421  \n",
       "187               0.000468  \n",
       "188              -0.000792  \n",
       "189               0.000365  \n",
       "190               0.000017  \n",
       "191              -0.000936  \n",
       "192              -0.000125  \n",
       "193              -0.000980  \n",
       "\n",
       "[194 rows x 7 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **the coefficients trained with L2 penalty 0**, find the 5 most positive words (with largest positive coefficients). Save them to **positive_words**. Similarly, find the 5 most negative words (with largest negative coefficients) and save them to **negative_words**.\n",
    "\n",
    "**Quiz Question**. Which of the following is **not** listed in either **positive_words** or **negative_words**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coefficients [L2=0]</th>\n",
       "      <th>coefficients [L2=4]</th>\n",
       "      <th>coefficients [L2=10]</th>\n",
       "      <th>coefficients [L2=1e2]</th>\n",
       "      <th>coefficients [L2=1e3]</th>\n",
       "      <th>coefficients [L2=1e5]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>love</td>\n",
       "      <td>1.058554</td>\n",
       "      <td>1.050856</td>\n",
       "      <td>1.039529</td>\n",
       "      <td>0.896644</td>\n",
       "      <td>0.418354</td>\n",
       "      <td>0.009042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>loves</td>\n",
       "      <td>1.052484</td>\n",
       "      <td>1.043903</td>\n",
       "      <td>1.031265</td>\n",
       "      <td>0.870794</td>\n",
       "      <td>0.345870</td>\n",
       "      <td>0.006150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>easy</td>\n",
       "      <td>0.984559</td>\n",
       "      <td>0.977600</td>\n",
       "      <td>0.967362</td>\n",
       "      <td>0.838245</td>\n",
       "      <td>0.401904</td>\n",
       "      <td>0.008808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>perfect</td>\n",
       "      <td>0.835693</td>\n",
       "      <td>0.828555</td>\n",
       "      <td>0.818038</td>\n",
       "      <td>0.684143</td>\n",
       "      <td>0.250614</td>\n",
       "      <td>0.003989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>great</td>\n",
       "      <td>0.801625</td>\n",
       "      <td>0.796897</td>\n",
       "      <td>0.789935</td>\n",
       "      <td>0.701425</td>\n",
       "      <td>0.376012</td>\n",
       "      <td>0.008950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word  coefficients [L2=0]  coefficients [L2=4]  coefficients [L2=10]  \\\n",
       "4      love             1.058554             1.050856              1.039529   \n",
       "23    loves             1.052484             1.043903              1.031265   \n",
       "8      easy             0.984559             0.977600              0.967362   \n",
       "34  perfect             0.835693             0.828555              0.818038   \n",
       "3     great             0.801625             0.796897              0.789935   \n",
       "\n",
       "    coefficients [L2=1e2]  coefficients [L2=1e3]  coefficients [L2=1e5]  \n",
       "4                0.896644               0.418354               0.009042  \n",
       "23               0.870794               0.345870               0.006150  \n",
       "8                0.838245               0.401904               0.008808  \n",
       "34               0.684143               0.250614               0.003989  \n",
       "3                0.701425               0.376012               0.008950  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_table = table.sort_values('coefficients [L2=0]', ascending=False)[0:5]\n",
    "new_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       love\n",
       "1      loves\n",
       "2       easy\n",
       "3    perfect\n",
       "4      great\n",
       "Name: word, dtype: object"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_words = new_table['word']\n",
    "positive_words.index = range(5)\n",
    "positive_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>coefficients [L2=0]</th>\n",
       "      <th>coefficients [L2=4]</th>\n",
       "      <th>coefficients [L2=10]</th>\n",
       "      <th>coefficients [L2=1e2]</th>\n",
       "      <th>coefficients [L2=1e3]</th>\n",
       "      <th>coefficients [L2=1e5]</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>disappointed</td>\n",
       "      <td>-0.955437</td>\n",
       "      <td>-0.946980</td>\n",
       "      <td>-0.934518</td>\n",
       "      <td>-0.775625</td>\n",
       "      <td>-0.266095</td>\n",
       "      <td>-0.004013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>money</td>\n",
       "      <td>-0.768793</td>\n",
       "      <td>-0.762734</td>\n",
       "      <td>-0.753818</td>\n",
       "      <td>-0.641406</td>\n",
       "      <td>-0.275883</td>\n",
       "      <td>-0.005487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>return</td>\n",
       "      <td>-0.742085</td>\n",
       "      <td>-0.735502</td>\n",
       "      <td>-0.725807</td>\n",
       "      <td>-0.602646</td>\n",
       "      <td>-0.215199</td>\n",
       "      <td>-0.003730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>waste</td>\n",
       "      <td>-0.617809</td>\n",
       "      <td>-0.612475</td>\n",
       "      <td>-0.604620</td>\n",
       "      <td>-0.505189</td>\n",
       "      <td>-0.190631</td>\n",
       "      <td>-0.003345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>returned</td>\n",
       "      <td>-0.572707</td>\n",
       "      <td>-0.567518</td>\n",
       "      <td>-0.559870</td>\n",
       "      <td>-0.462056</td>\n",
       "      <td>-0.150021</td>\n",
       "      <td>-0.002225</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word  coefficients [L2=0]  coefficients [L2=4]  \\\n",
       "106  disappointed            -0.955437            -0.946980   \n",
       "97          money            -0.768793            -0.762734   \n",
       "114        return            -0.742085            -0.735502   \n",
       "113         waste            -0.617809            -0.612475   \n",
       "169      returned            -0.572707            -0.567518   \n",
       "\n",
       "     coefficients [L2=10]  coefficients [L2=1e2]  coefficients [L2=1e3]  \\\n",
       "106             -0.934518              -0.775625              -0.266095   \n",
       "97              -0.753818              -0.641406              -0.275883   \n",
       "114             -0.725807              -0.602646              -0.215199   \n",
       "113             -0.604620              -0.505189              -0.190631   \n",
       "169             -0.559870              -0.462056              -0.150021   \n",
       "\n",
       "     coefficients [L2=1e5]  \n",
       "106              -0.004013  \n",
       "97               -0.005487  \n",
       "114              -0.003730  \n",
       "113              -0.003345  \n",
       "169              -0.002225  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_table_2 = table.sort_values('coefficients [L2=0]', ascending=True)[0:5]\n",
    "new_table_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    disappointed\n",
       "1           money\n",
       "2          return\n",
       "3           waste\n",
       "4        returned\n",
       "Name: word, dtype: object"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_words = new_table_2['word']\n",
    "negative_words.index = range(5)\n",
    "negative_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14.. Let us observe the effect of increasing L2 penalty on the 10 words just selected. Make a plot of the coefficients for the 10 words over the different values of L2 penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hints:\n",
    "\n",
    "First, extract rows corresponding to positive_words. Do the same for negative_words.\n",
    "Then plot each of the extracted rows. The x axis should be L2 penalty and the y axis should be the coefficient value.\n",
    "Use log scale for the x axis, as the L2 penalty values are exponentially spaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 10, 6\n",
    "\n",
    "def make_coefficient_plot(table, positive_words, negative_words, l2_penalty_list):\n",
    "    cmap_positive = plt.get_cmap('Reds')\n",
    "    cmap_negative = plt.get_cmap('Blues')\n",
    "    \n",
    "    xx = l2_penalty_list\n",
    "    plt.plot(xx, [0.]*len(xx), '--', lw=1, color='k')\n",
    "    \n",
    "    table_positive_words = table[table['word'].isin(positive_words)]\n",
    "    table_negative_words = table[table['word'].isin(negative_words)]\n",
    "    del table_positive_words['word']\n",
    "    del table_negative_words['word']\n",
    "    \n",
    "    for i in range(len(positive_words)):\n",
    "        color = cmap_positive(0.8*((i+1)/(len(positive_words)*1.2)+0.15))\n",
    "        plt.plot(xx, table_positive_words[i:i+1].values.flatten(),\n",
    "                 '-', label=positive_words[i], linewidth=4.0, color=color)\n",
    "        \n",
    "    for i in range(len(negative_words)):\n",
    "        color = cmap_negative(0.8*((i+1)/(len(negative_words)*1.2)+0.15))\n",
    "        plt.plot(xx, table_negative_words[i:i+1].values.flatten(),\n",
    "                 '-', label=negative_words[i], linewidth=4.0, color=color)\n",
    "        \n",
    "    plt.legend(loc='best', ncol=3, prop={'size':16}, columnspacing=0.5)\n",
    "    plt.axis([1, 1e5, -1, 2])\n",
    "    plt.title('Coefficient path')\n",
    "    plt.xlabel('L2 penalty ($\\lambda$)')\n",
    "    plt.ylabel('Coefficient value')\n",
    "    plt.xscale('log')\n",
    "    plt.rcParams.update({'font.size': 18})\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArcAAAGXCAYAAACp2XjcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlsdTrHgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a10ef8048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#make_coefficient_plot(table, positive_words, negative_words, l2_penalty_list=[0, 4, 10, 1e2, 1e3, 1e5])\n",
    "l2_penalty_list=[0, 4, 10, 1e2, 1e3, 1e5]\n",
    "cmap_positive = plt.get_cmap('Reds')\n",
    "cmap_negative = plt.get_cmap('Blues')\n",
    "\n",
    "xx = l2_penalty_list\n",
    "plt.plot(xx, [0.]*len(xx), '--', lw=1, color='k')\n",
    "\n",
    "table_positive_words = table[table['word'].isin(positive_words)]\n",
    "table_negative_words = table[table['word'].isin(negative_words)]\n",
    "del table_positive_words['word']\n",
    "del table_negative_words['word']\n",
    "\n",
    "for i in range(len(positive_words)):\n",
    "    color = cmap_positive(0.8*((i+1)/(len(positive_words)*1.2)+0.15))\n",
    "    plt.plot(xx, table_positive_words[i:i+1].values.flatten(),\n",
    "             '-', label=positive_words[i], linewidth=4.0, color=color)\n",
    "\n",
    "for i in range(len(negative_words)):\n",
    "    color = cmap_negative(0.8*((i+1)/(len(negative_words)*1.2)+0.15))\n",
    "    plt.plot(xx, table_negative_words[i:i+1].values.flatten(),\n",
    "             '-', label=negative_words[i], linewidth=4.0, color=color)\n",
    "\n",
    "plt.legend(loc='best', ncol=3, prop={'size':16}, columnspacing=0.5)\n",
    "plt.axis([1, 1e5, -1, 2])\n",
    "plt.title('Coefficient path')\n",
    "plt.xlabel('L2 penalty ($\\lambda$)')\n",
    "plt.ylabel('Coefficient value')\n",
    "plt.xscale('log')\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: (True/False) All coefficients consistently get smaller in size as the L2 penalty is increased.\n",
    "\n",
    "\n",
    "**Quiz Question**: (True/False) The relative order of coefficients is preserved as the L2 penalty is increased. (For example, if the coefficient for 'cat' was more positive than that for 'dog', this remains true as the L2 penalty increases.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is True\n",
      "The answer is False\n"
     ]
    }
   ],
   "source": [
    "print('The answer is True')\n",
    "print('The answer is False')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring accuracy\n",
    "\n",
    "Now, let us compute the accuracy of the classifier model. Recall that the accuracy is given by\n",
    "\n",
    "$$\n",
    "\\mbox{accuracy} = \\frac{\\mbox{# correctly classified data points}}{\\mbox{# total data points}}\n",
    "$$\n",
    "\n",
    "\n",
    "Recall from lecture that that the class prediction is calculated using\n",
    "$$\n",
    "\\hat{y}_i = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      +1 & h(\\mathbf{x}_i)^T\\mathbf{w} > 0 \\\\\n",
    "      -1 & h(\\mathbf{x}_i)^T\\mathbf{w} \\leq 0 \\\\\n",
    "\\end{array} \n",
    "\\right.\n",
    "$$\n",
    "\n",
    "**Note**: It is important to know that the model prediction code doesn't change even with the addition of an L2 penalty. The only thing that changes is the estimated coefficients used in this prediction.\n",
    "\n",
    "Based on the above, we will use the same code that was used in Module 3 assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_classification_accuracy(feature_matrix, sentiment, coefficients):\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    apply_threshold = np.vectorize(lambda x: 1. if x > 0  else -1.)\n",
    "    predictions = apply_threshold(scores)\n",
    "    \n",
    "    num_correct = (predictions == sentiment).sum()\n",
    "    accuracy = num_correct / len(feature_matrix)    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_accuracy = {}\n",
    "train_accuracy[0]   = get_classification_accuracy(feature_matrix_train, sentiment_train, coefficients_0_penalty)\n",
    "train_accuracy[4]   = get_classification_accuracy(feature_matrix_train, sentiment_train, coefficients_4_penalty)\n",
    "train_accuracy[10]  = get_classification_accuracy(feature_matrix_train, sentiment_train, coefficients_10_penalty)\n",
    "train_accuracy[1e2] = get_classification_accuracy(feature_matrix_train, sentiment_train, coefficients_1e2_penalty)\n",
    "train_accuracy[1e3] = get_classification_accuracy(feature_matrix_train, sentiment_train, coefficients_1e3_penalty)\n",
    "train_accuracy[1e5] = get_classification_accuracy(feature_matrix_train, sentiment_train, coefficients_1e5_penalty)\n",
    "\n",
    "validation_accuracy = {}\n",
    "validation_accuracy[0]   = get_classification_accuracy(feature_matrix_valid, sentiment_valid, coefficients_0_penalty)\n",
    "validation_accuracy[4]   = get_classification_accuracy(feature_matrix_valid, sentiment_valid, coefficients_4_penalty)\n",
    "validation_accuracy[10]  = get_classification_accuracy(feature_matrix_valid, sentiment_valid, coefficients_10_penalty)\n",
    "validation_accuracy[1e2] = get_classification_accuracy(feature_matrix_valid, sentiment_valid, coefficients_1e2_penalty)\n",
    "validation_accuracy[1e3] = get_classification_accuracy(feature_matrix_valid, sentiment_valid, coefficients_1e3_penalty)\n",
    "validation_accuracy[1e5] = get_classification_accuracy(feature_matrix_valid, sentiment_valid, coefficients_1e5_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 penalty = 0\n",
      "train accuracy = 0.785156157787, validation_accuracy = 0.78143964149\n",
      "--------------------------------------------------------------------------------\n",
      "L2 penalty = 4\n",
      "train accuracy = 0.785108944548, validation_accuracy = 0.781533003454\n",
      "--------------------------------------------------------------------------------\n",
      "L2 penalty = 10\n",
      "train accuracy = 0.784990911452, validation_accuracy = 0.781719727383\n",
      "--------------------------------------------------------------------------------\n",
      "L2 penalty = 100\n",
      "train accuracy = 0.783975826822, validation_accuracy = 0.781066193633\n",
      "--------------------------------------------------------------------------------\n",
      "L2 penalty = 1000\n",
      "train accuracy = 0.775855149784, validation_accuracy = 0.771356549342\n",
      "--------------------------------------------------------------------------------\n",
      "L2 penalty = 100000\n",
      "train accuracy = 0.680366374731, validation_accuracy = 0.667818130893\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Build a simple report\n",
    "for key in sorted(validation_accuracy.keys()):\n",
    "    print (\"L2 penalty = %g\" % key)\n",
    "    print (\"train accuracy = %s, validation_accuracy = %s\" % (train_accuracy[key], validation_accuracy[key]))\n",
    "    print (\"--------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.tight_layout>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmkAAAFtCAYAAABcCP1ZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIYU0ERERkTT0/wEO+1n+2f0AAAACSURBVJvQaJ08vwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a11224860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Optional. Plot accuracy on training and validation sets over choice of L2 penalty.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = 10, 6\n",
    "\n",
    "sorted_list = sorted(train_accuracy.items(), key=lambda x:x[0])\n",
    "plt.plot([p[0] for p in sorted_list], [p[1] for p in sorted_list], 'bo-', linewidth=4, label='Training accuracy')\n",
    "sorted_list = sorted(validation_accuracy.items(), key=lambda x:x[0])\n",
    "plt.plot([p[0] for p in sorted_list], [p[1] for p in sorted_list], 'ro-', linewidth=4, label='Validation accuracy')\n",
    "plt.xscale('symlog')\n",
    "plt.axis([0, 1e3, 0.78, 0.786])\n",
    "plt.legend(loc='lower left')\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "plt.tight_layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Quiz Question**: Which model (L2 = 0, 4, 10, 100, 1e3, 1e5) has the **highest** accuracy on the **training** data?\n",
    "* **Quiz Question**: Which model (L2 = 0, 4, 10, 100, 1e3, 1e5) has the **highest** accuracy on the **validation** data?\n",
    "* **Quiz Question**: Does the **highest** accuracy on the **training** data imply that the model is the best one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer: the highest accuracy on the training data: L2 penalty = 0\n",
      "The answer: the highest accuracy on the validation data: L2 penalty = 10\n",
      "The answer: no\n"
     ]
    }
   ],
   "source": [
    "print('The answer: the highest accuracy on the training data: L2 penalty = 0')\n",
    "print('The answer: the highest accuracy on the validation data: L2 penalty = 10')\n",
    "print('The answer: no')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
